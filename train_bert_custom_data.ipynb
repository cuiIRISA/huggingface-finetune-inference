{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ee062d-5b1a-4999-9105-525ecb44193f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97448ccb-9e3c-4798-a681-95f418adb2ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./dataset/sem_eval_2018_task_1_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7122d5-1603-4c8f-95c4-0fe19f378501",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16472513-d4cc-4560-a0e6-3a4225478d51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0ab9fe-4805-4034-96dc-47d0c7e4b7e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0473a485-dfda-4feb-8616-a3c38b838a0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_en = df.loc[:,[\"Tweet\"]]\n",
    "data_en.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742db28a-87aa-422d-b9c9-6f23360c32e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_en.values.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fea0e28-cb3c-4dfc-b100-f67bb056b798",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42a0105-ad1a-4b60-9b70-876f4c87b2a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_index = df.iloc[:,2:].astype(int)\n",
    "label_index.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420dae6c-7525-4af2-8f1c-a6bae3d4721a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_one_hot = label_index.to_numpy()\n",
    "label_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d537be96-e776-4f6c-b009-f8590b10c6b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/training\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f3af8c-89fb-4c2c-8a6b-43f5d2133943",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4f274c-8d41-4b3e-9681-64714fded2f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dc04ca-c24d-4b22-9589-7e1db8719617",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyNLPDataset(Dataset):\n",
    "    def __init__(self, file_name, model_name):\n",
    "        # data loading\n",
    "        df = pd.read_csv(file_name)\n",
    "        self.data_en = df.loc[:,[\"Tweet\"]].squeeze().values.tolist()\n",
    "        self.label_index = torch.from_numpy(df.iloc[:,2:].astype(int).to_numpy())\n",
    "        self.n_samples = df.shape[0]\n",
    "        self.embedding_model = None\n",
    "        self.tokenizer = None\n",
    "        self.model_name = model_name\n",
    "        self.tokenized_text = [ {} for i in range(df.shape[0])]\n",
    "        #self.token_type_ids = None\n",
    "        #self.attention_mask = None\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.tokenized_text[index] == {}:\n",
    "            if self.tokenizer == None:\n",
    "                self.load_embedding_model()\n",
    "            \n",
    "            text = self.data_en[index]\n",
    "            assert isinstance(text, str)\n",
    "            text_tokenized = self.tokenize_function(text)\n",
    "            self.tokenized_text[index] = text_tokenized\n",
    "        #return self.tokenized_text[index]\n",
    "        return self.tokenized_text[index], self.label_index[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def load_embedding_model(self):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
    "    \n",
    "    def tokenize_function(self,text):\n",
    "        # generate token from dataset \n",
    "        tokenized_text = self.tokenizer(text, max_length = 128, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        tokenized_text['input_ids'] = torch.squeeze(tokenized_text['input_ids'])\n",
    "        tokenized_text['token_type_ids'] = torch.squeeze(tokenized_text['token_type_ids'])\n",
    "        tokenized_text['attention_mask'] = torch.squeeze(tokenized_text['attention_mask'])\n",
    "        \n",
    "        return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1691ae84-585a-42e0-bd14-22221d3b89ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = MyNLPDataset(\"./dataset/sem_eval_2018_task_1_train.csv\", \"bert-base-multilingual-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693e456a-e518-44ca-8502-e326dd755c66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#text = \"Hello Serena\"\n",
    "#tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "#tokenized_text = tokenizer(text, max_length = 128, padding=\"max_length\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82029db-ccb9-46d4-8007-3fb746e847d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1249603-9471-4733-ad86-bddad1626b42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_dataset = MyNLPDataset(\"./dataset/sem_eval_2018_task_1_validation.csv\", \"bert-base-multilingual-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f49165b-3e27-4f53-b2d4-0ac662b7d221",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_count = len(train_dataset)\n",
    "validation_count = len(validation_dataset)\n",
    "print(training_count, validation_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ed0afe-7308-47b9-9bbf-8cd3137f4223",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "validation_dataloader = DataLoader(dataset=validation_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cc9ca1-f1be-4935-8b3f-72e473ef3c04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_iter = iter(train_dataloader)\n",
    "inputs, labels = next(data_iter)\n",
    "labels = labels.type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47daf945-d778-4594-ba2c-cd52b6b7e18e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b920fe55-bb64-42c2-8bff-418548e2b838",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bd66f7-cf65-4d3c-95fd-a4e5b56afc79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PyTorch models inherit from torch.nn.Module\n",
    "class SentenceMultiClassClassifier(nn.Module):\n",
    "    def __init__(self,number_class, pretrained_model):\n",
    "        super(SentenceMultiClassClassifier, self).__init__()\n",
    "        self.number_class = number_class\n",
    "        self.pretrained = BertModel.from_pretrained(pretrained_model)\n",
    "        #self.pretrained = BertModel.from_pretrained(pretrained_model,config=AutoConfig.from_pretrained(pretrained_model, output_attentions=True,output_hidden_states=True))\n",
    "\n",
    "        #self.dropout = nn.Dropout(0.5) \n",
    "        #self.fc1 = nn.Linear(768, 1200)\n",
    "        #self.fc2 = nn.Linear(1200, 1400)\n",
    "        #self.fc3 = nn.Linear(1400, number_class)\n",
    "        \n",
    "        self.linear = nn.Linear(768, number_class)\n",
    "        self.layeroutput = torch.nn.Sigmoid()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask):            \n",
    "        output_pretrained = self.pretrained(input_ids, token_type_ids, attention_mask)\n",
    "        # Freeze the BERT parameters\n",
    "        #for param in self.pretrained.parameters():\n",
    "        #    param.requires_grad = False\n",
    "            \n",
    "        #x = F.relu(self.fc1(output_pretrained.last_hidden_state[:,0,:].view(-1,768)))\n",
    "        #x = self.dropout(x)\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        #x = self.dropout(x)\n",
    "        #x = output_pretrained.last_hidden_state[:,0,:].view(-1,768)\n",
    "        \n",
    "        x = output_pretrained.pooler_output\n",
    "        x = self.linear(x)\n",
    "        x = self.layeroutput(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231494b8-fae0-4a82-8a0b-60bc73729a93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer, BertModel\n",
    "# import torch\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "# model = BertModel.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "\n",
    "# inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "# print(inputs)\n",
    "# labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "# pooler_output = model(**inputs).pooler_output\n",
    "# output_hidden = model(**inputs).last_hidden_state[:,0,:].view(-1,768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8f8177-cfdf-4bc5-8d0a-1f29f0d91da4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_CLASS = 11\n",
    "PRETRAINED_MODEL = \"bert-base-multilingual-uncased\"\n",
    "\n",
    "model = SentenceMultiClassClassifier(NUM_CLASS, PRETRAINED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3dd0f6-c286-48dd-9fc7-cd719a5f001d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d56eedd-c27e-4ded-9bac-7dfb1adc07fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = labels.type(torch.FloatTensor)\n",
    "labels = labels.to(device)\n",
    "for key, value in inputs.items():\n",
    "    inputs[key] = inputs[key].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9277e111-e414-43a7-b0ee-dcaacdb0cdce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.to(device)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f343c4-c2f9-42cf-82fa-b65e9cba29f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2935e6d6-d60b-4a92-8970-ed9df452b2ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCELoss()\n",
    "# Optimizers specified in the torch.optim package\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d880ad1-e8d0-4aa9-9566-bb1d2306243c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute the loss and its gradients\n",
    "loss = loss_fn(outputs, labels)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a8b1cb-eb67-4bf4-ba41-7cb49f9ea821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09ca455-24b5-45b2-8a3c-26508fff7c76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index,training_loader, optimizer, model, loss_fn, device):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    batch_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "        labels = labels.type(torch.FloatTensor)\n",
    "        labels = labels.to(device)\n",
    "        for key, value in inputs.items():\n",
    "            inputs[key] = inputs[key].to(device)\n",
    "            \n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        batch_loss += loss.item()\n",
    "        if i % 10 == 9:\n",
    "            last_loss = running_loss / 10 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            running_loss = 0.\n",
    "           \n",
    "    return batch_loss / len(training_loader)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2a4a17-08bd-4b85-b4d5-58ee11673a43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_one_epoch(0,train_dataloader, optimizer, model, loss_fn, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46472bca-d8f1-4d1f-a958-0647ee148b93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c2f73e-215f-4d8d-8ec1-5d7353d4683c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "EPOCHS = 2\n",
    "epoch_number = 0\n",
    "best_vloss = 1_000_000.\n",
    "MODEL_SAVE_LOCATION = \"./model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40c6a47-7bd5-43b8-a6c0-9a6b89a01731",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, train_dataloader, optimizer, model, loss_fn, device)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    #model.train(False)\n",
    "    model.eval()\n",
    "    running_vloss = 0.0\n",
    "\n",
    "    for i, vdata in enumerate(validation_dataloader):\n",
    "        vinputs, vlabels = vdata\n",
    "        vlabels = vlabels.type(torch.FloatTensor)\n",
    "        vlabels = vlabels.to(device)\n",
    "        for key, value in vinputs.items():\n",
    "            vinputs[key] = vinputs[key].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            voutputs = model(**vinputs)\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = '{}/model.pth'.format(MODEL_SAVE_LOCATION)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc352d56-ae99-4047-84a5-4411c5c974fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fbe987-b035-4a1e-8532-37cc64552231",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad4d2af-6324-4400-86c9-7062742ea2c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7688510-0b93-440f-878a-40188afb029b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training ...\n",
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "EPOCH 1:\n",
      "  batch 10 loss: 0.5953069925308228\n",
      "  batch 20 loss: 0.5219672858715058\n",
      "  batch 30 loss: 0.4847619473934174\n",
      "  batch 40 loss: 0.4811796396970749\n",
      "  batch 50 loss: 0.4838826060295105\n",
      "  batch 60 loss: 0.46915295720100403\n",
      "  batch 70 loss: 0.4783048003911972\n",
      "  batch 80 loss: 0.47761703431606295\n",
      "  batch 90 loss: 0.48256914019584657\n",
      "  batch 100 loss: 0.46292818784713746\n",
      "  batch 110 loss: 0.48607408106327055\n",
      "  batch 120 loss: 0.45796370804309844\n",
      "  batch 130 loss: 0.47426059246063235\n",
      "  batch 140 loss: 0.47302416563034055\n",
      "  batch 150 loss: 0.46723059713840487\n",
      "  batch 160 loss: 0.48939109146595\n",
      "  batch 170 loss: 0.46745731830596926\n",
      "  batch 180 loss: 0.46104940176010134\n",
      "  batch 190 loss: 0.4627549409866333\n",
      "  batch 200 loss: 0.46095885932445524\n",
      "  batch 210 loss: 0.4805862843990326\n",
      "LOSS train 0.4814644645307666 valid 0.4777849614620209\n",
      "EPOCH 2:\n",
      "  batch 10 loss: 0.46337033808231354\n",
      "  batch 20 loss: 0.4766377121210098\n",
      "  batch 30 loss: 0.48417857587337493\n",
      "  batch 40 loss: 0.46773006618022916\n",
      "  batch 50 loss: 0.4787057310342789\n",
      "  batch 60 loss: 0.4616753578186035\n",
      "  batch 70 loss: 0.4533413082361221\n",
      "  batch 80 loss: 0.4652659177780151\n",
      "  batch 90 loss: 0.46605072617530824\n",
      "  batch 100 loss: 0.47197040617465974\n",
      "  batch 110 loss: 0.46389244496822357\n",
      "  batch 120 loss: 0.4599153369665146\n",
      "  batch 130 loss: 0.4772739440202713\n",
      "  batch 140 loss: 0.4670494943857193\n",
      "  batch 150 loss: 0.4744676649570465\n",
      "  batch 160 loss: 0.47067752182483674\n",
      "  batch 170 loss: 0.48437815308570864\n",
      "  batch 180 loss: 0.4667145639657974\n",
      "  batch 190 loss: 0.462770140171051\n",
      "  batch 200 loss: 0.4723225861787796\n",
      "  batch 210 loss: 0.46628296971321104\n",
      "LOSS train 0.4696892527776344 valid 0.47066035866737366\n",
      "EPOCH 3:\n",
      "  batch 10 loss: 0.47035550475120547\n",
      "  batch 20 loss: 0.4675004601478577\n",
      "  batch 30 loss: 0.46679922938346863\n",
      "  batch 40 loss: 0.46666627526283266\n",
      "  batch 50 loss: 0.4636185497045517\n",
      "  batch 60 loss: 0.4679271697998047\n",
      "  batch 70 loss: 0.46281746327877044\n",
      "  batch 80 loss: 0.47173182368278505\n",
      "  batch 90 loss: 0.4564757853746414\n",
      "  batch 100 loss: 0.4805025517940521\n",
      "  batch 110 loss: 0.4731439560651779\n",
      "  batch 120 loss: 0.4646448940038681\n",
      "  batch 130 loss: 0.4570533484220505\n",
      "  batch 140 loss: 0.4733643800020218\n",
      "  batch 150 loss: 0.4601444512605667\n",
      "  batch 160 loss: 0.4796407222747803\n",
      "  batch 170 loss: 0.4677686482667923\n",
      "  batch 180 loss: 0.49643097519874574\n",
      "  batch 190 loss: 0.47946870028972627\n",
      "  batch 200 loss: 0.46252964437007904\n",
      "  batch 210 loss: 0.4730104893445969\n",
      "LOSS train 0.4699651192281848 valid 0.47487562894821167\n",
      "EPOCH 4:\n",
      "  batch 10 loss: 0.47601615488529203\n",
      "  batch 20 loss: 0.47257170379161834\n",
      "  batch 30 loss: 0.46969579756259916\n",
      "  batch 40 loss: 0.47832030057907104\n",
      "  batch 50 loss: 0.47017765045166016\n",
      "  batch 60 loss: 0.45860028862953184\n",
      "  batch 70 loss: 0.448779296875\n",
      "  batch 80 loss: 0.46579497754573823\n",
      "  batch 90 loss: 0.47188825607299806\n",
      "  batch 100 loss: 0.45520401895046236\n",
      "  batch 110 loss: 0.4856992900371552\n",
      "  batch 120 loss: 0.4745289355516434\n",
      "  batch 130 loss: 0.4807374805212021\n",
      "  batch 140 loss: 0.47402421832084657\n",
      "  batch 150 loss: 0.4505134642124176\n",
      "  batch 160 loss: 0.45684232413768766\n",
      "  batch 170 loss: 0.47171485126018525\n",
      "  batch 180 loss: 0.45823825895786285\n",
      "  batch 190 loss: 0.4531928598880768\n",
      "  batch 200 loss: 0.47673249542713164\n",
      "  batch 210 loss: 0.47070426642894747\n",
      "LOSS train 0.467758152529458 valid 0.47528013586997986\n",
      "EPOCH 5:\n",
      "  batch 10 loss: 0.4751215934753418\n",
      "  batch 20 loss: 0.47215135097503663\n",
      "  batch 30 loss: 0.44766293168067933\n",
      "  batch 40 loss: 0.4698341578245163\n",
      "  batch 50 loss: 0.4671232163906097\n",
      "  batch 60 loss: 0.45076207220554354\n",
      "  batch 70 loss: 0.4682334721088409\n",
      "  batch 80 loss: 0.473834764957428\n",
      "  batch 90 loss: 0.4728625982999802\n",
      "  batch 100 loss: 0.46337843537330625\n",
      "  batch 110 loss: 0.4544934332370758\n",
      "  batch 120 loss: 0.4753767251968384\n",
      "  batch 130 loss: 0.46872947216033933\n",
      "  batch 140 loss: 0.46771016120910647\n",
      "  batch 150 loss: 0.4740340828895569\n",
      "  batch 160 loss: 0.4681500196456909\n",
      "  batch 170 loss: 0.4587994933128357\n",
      "  batch 180 loss: 0.4690848350524902\n",
      "  batch 190 loss: 0.46742006838321687\n",
      "  batch 200 loss: 0.48473200500011443\n",
      "  batch 210 loss: 0.4898596167564392\n",
      "LOSS train 0.4689761322235393 valid 0.47436803579330444\n",
      "EPOCH 6:\n",
      "  batch 10 loss: 0.4734645545482635\n",
      "  batch 20 loss: 0.4667925536632538\n",
      "  batch 30 loss: 0.4736595243215561\n",
      "  batch 40 loss: 0.46106907427310945\n",
      "  batch 50 loss: 0.4722651928663254\n",
      "  batch 60 loss: 0.4726407080888748\n",
      "  batch 70 loss: 0.47167785465717316\n",
      "  batch 80 loss: 0.47252386808395386\n",
      "  batch 90 loss: 0.46815532743930816\n",
      "  batch 100 loss: 0.47952345907688143\n",
      "  batch 110 loss: 0.4577025592327118\n",
      "  batch 120 loss: 0.472488272190094\n",
      "  batch 130 loss: 0.46619769632816316\n",
      "  batch 140 loss: 0.4722288906574249\n",
      "  batch 150 loss: 0.47433638870716094\n",
      "  batch 160 loss: 0.460330867767334\n",
      "  batch 170 loss: 0.4706129342317581\n",
      "  batch 180 loss: 0.47827966809272765\n",
      "  batch 190 loss: 0.45960805714130404\n",
      "  batch 200 loss: 0.4688272088766098\n",
      "  batch 210 loss: 0.47199413776397703\n",
      "LOSS train 0.4698585773182807 valid 0.47383561730384827\n",
      "EPOCH 7:\n",
      "  batch 10 loss: 0.45829170644283296\n",
      "  batch 20 loss: 0.47557038366794585\n",
      "  batch 30 loss: 0.4623280942440033\n",
      "  batch 40 loss: 0.45726544559001925\n",
      "  batch 50 loss: 0.46185815930366514\n",
      "  batch 60 loss: 0.47642347812652586\n",
      "  batch 70 loss: 0.48067550659179686\n",
      "  batch 80 loss: 0.46805221438407896\n",
      "  batch 90 loss: 0.4662086844444275\n",
      "  batch 100 loss: 0.47005332708358766\n",
      "  batch 110 loss: 0.4684059113264084\n",
      "  batch 120 loss: 0.45916090309619906\n",
      "  batch 130 loss: 0.4746803194284439\n",
      "  batch 140 loss: 0.4762417942285538\n",
      "  batch 150 loss: 0.46826739609241486\n",
      "  batch 160 loss: 0.4741496741771698\n",
      "  batch 170 loss: 0.4732179582118988\n",
      "  batch 180 loss: 0.46534780859947206\n",
      "  batch 190 loss: 0.4774418234825134\n",
      "  batch 200 loss: 0.46110119521617887\n",
      "  batch 210 loss: 0.45044465363025665\n",
      "LOSS train 0.4675581451052817 valid 0.4543205499649048\n",
      "EPOCH 8:\n",
      "  batch 10 loss: 0.4648851215839386\n",
      "  batch 20 loss: 0.4676384925842285\n",
      "  batch 30 loss: 0.48479649126529695\n",
      "  batch 40 loss: 0.46523325741291044\n",
      "  batch 50 loss: 0.4693356603384018\n",
      "  batch 60 loss: 0.44911439120769503\n",
      "  batch 70 loss: 0.4516456097364426\n",
      "  batch 80 loss: 0.460880783200264\n",
      "  batch 90 loss: 0.47038892209529876\n",
      "  batch 100 loss: 0.4521548688411713\n",
      "  batch 110 loss: 0.46188784539699557\n",
      "  batch 120 loss: 0.4753179460763931\n",
      "  batch 130 loss: 0.4723729878664017\n",
      "  batch 140 loss: 0.47160891592502596\n",
      "  batch 150 loss: 0.45595194697380065\n",
      "  batch 160 loss: 0.4735522776842117\n",
      "  batch 170 loss: 0.4822244018316269\n",
      "  batch 180 loss: 0.4660657078027725\n",
      "  batch 190 loss: 0.46248806416988375\n",
      "  batch 200 loss: 0.4768242686986923\n",
      "  batch 210 loss: 0.45180869698524473\n",
      "LOSS train 0.4658261820256153 valid 0.4794110655784607\n",
      "EPOCH 9:\n",
      "  batch 10 loss: 0.4601354420185089\n",
      "  batch 20 loss: 0.46809602081775664\n",
      "  batch 30 loss: 0.4615461975336075\n",
      "  batch 40 loss: 0.46106095612049103\n",
      "  batch 50 loss: 0.4718777060508728\n",
      "  batch 60 loss: 0.4576334536075592\n",
      "  batch 70 loss: 0.4537572801113129\n",
      "  batch 80 loss: 0.4733343869447708\n",
      "  batch 90 loss: 0.44785116612911224\n",
      "  batch 100 loss: 0.47693763077259066\n",
      "  batch 110 loss: 0.45467603504657744\n",
      "  batch 120 loss: 0.4530556559562683\n",
      "  batch 130 loss: 0.4669558256864548\n",
      "  batch 140 loss: 0.4557034969329834\n",
      "  batch 150 loss: 0.4613573431968689\n",
      "  batch 160 loss: 0.4683712750673294\n",
      "  batch 170 loss: 0.48658653199672697\n",
      "  batch 180 loss: 0.4602180063724518\n",
      "  batch 190 loss: 0.47872963845729827\n",
      "  batch 200 loss: 0.4642461210489273\n",
      "  batch 210 loss: 0.4709125101566315\n",
      "LOSS train 0.46458708543643773 valid 0.46418875455856323\n",
      "EPOCH 10:\n",
      "  batch 10 loss: 0.45425769984722136\n",
      "  batch 20 loss: 0.452277809381485\n",
      "  batch 30 loss: 0.4622469902038574\n",
      "  batch 40 loss: 0.4579608738422394\n",
      "  batch 50 loss: 0.4537081182003021\n",
      "  batch 60 loss: 0.4731599748134613\n",
      "  batch 70 loss: 0.4491740494966507\n",
      "  batch 80 loss: 0.4499561578035355\n",
      "  batch 90 loss: 0.478225576877594\n",
      "  batch 100 loss: 0.4481159746646881\n",
      "  batch 110 loss: 0.4622869580984116\n",
      "  batch 120 loss: 0.4672922194004059\n",
      "  batch 130 loss: 0.439781528711319\n",
      "  batch 140 loss: 0.4834184139966965\n",
      "  batch 150 loss: 0.4914282411336899\n",
      "  batch 160 loss: 0.4772348016500473\n",
      "  batch 170 loss: 0.4526293456554413\n",
      "  batch 180 loss: 0.46098717451095583\n",
      "  batch 190 loss: 0.45526730120182035\n",
      "  batch 200 loss: 0.44918741285800934\n",
      "  batch 210 loss: 0.4746546000242233\n",
      "LOSS train 0.4616342628392104 valid 0.48717284202575684\n",
      "EPOCH 11:\n",
      "  batch 10 loss: 0.47295000553131106\n",
      "  batch 20 loss: 0.4651642471551895\n",
      "  batch 30 loss: 0.47244592010974884\n",
      "  batch 40 loss: 0.4633454442024231\n",
      "  batch 50 loss: 0.47580954134464265\n",
      "  batch 60 loss: 0.46388196051120756\n",
      "  batch 70 loss: 0.469311460852623\n",
      "  batch 80 loss: 0.4848118215799332\n",
      "  batch 90 loss: 0.4666271358728409\n",
      "  batch 100 loss: 0.47400716245174407\n",
      "  batch 110 loss: 0.45657904744148253\n",
      "  batch 120 loss: 0.4599152058362961\n",
      "  batch 130 loss: 0.47885640561580656\n",
      "  batch 140 loss: 0.4654856503009796\n",
      "  batch 150 loss: 0.46153219044208527\n",
      "  batch 160 loss: 0.4780439019203186\n",
      "  batch 170 loss: 0.4772512584924698\n",
      "  batch 180 loss: 0.47772051990032194\n",
      "  batch 190 loss: 0.46150617897510526\n",
      "  batch 200 loss: 0.47169086039066316\n",
      "  batch 210 loss: 0.4584871917963028\n",
      "LOSS train 0.4693422236732233 valid 0.45245909690856934\n",
      "EPOCH 12:\n",
      "  batch 10 loss: 0.4512467384338379\n",
      "  batch 20 loss: 0.47891308963298795\n",
      "  batch 30 loss: 0.4670490652322769\n",
      "  batch 40 loss: 0.45917455554008485\n",
      "  batch 50 loss: 0.4604943871498108\n",
      "  batch 60 loss: 0.4693929523229599\n",
      "  batch 70 loss: 0.45543734133243563\n",
      "  batch 80 loss: 0.4685765266418457\n",
      "  batch 90 loss: 0.45441638827323916\n",
      "  batch 100 loss: 0.450845804810524\n",
      "  batch 110 loss: 0.4793906629085541\n",
      "  batch 120 loss: 0.47503438889980315\n",
      "  batch 130 loss: 0.47895103991031646\n",
      "  batch 140 loss: 0.4654231071472168\n",
      "  batch 150 loss: 0.47051956951618196\n",
      "  batch 160 loss: 0.47918803691864015\n",
      "  batch 170 loss: 0.47083178758621214\n",
      "  batch 180 loss: 0.46029359102249146\n",
      "  batch 190 loss: 0.473710423707962\n",
      "  batch 200 loss: 0.477009779214859\n",
      "  batch 210 loss: 0.4817316919565201\n",
      "LOSS train 0.4679901613810352 valid 0.47316110134124756\n",
      "EPOCH 13:\n",
      "  batch 10 loss: 0.4613973706960678\n",
      "  batch 20 loss: 0.4734437674283981\n",
      "  batch 30 loss: 0.4715146660804749\n",
      "  batch 40 loss: 0.48505968153476714\n",
      "  batch 50 loss: 0.4772475063800812\n",
      "  batch 60 loss: 0.46929306387901304\n",
      "  batch 70 loss: 0.474520742893219\n",
      "  batch 80 loss: 0.4710576325654984\n",
      "  batch 90 loss: 0.47133103013038635\n",
      "  batch 100 loss: 0.4624074220657349\n",
      "  batch 110 loss: 0.4669268727302551\n",
      "  batch 120 loss: 0.47495933771133425\n",
      "  batch 130 loss: 0.4616639107465744\n",
      "  batch 140 loss: 0.47564847469329835\n",
      "  batch 150 loss: 0.47362915575504305\n",
      "  batch 160 loss: 0.46022911965847013\n",
      "  batch 170 loss: 0.4655791312456131\n",
      "  batch 180 loss: 0.47267589867115023\n",
      "  batch 190 loss: 0.46867499947547914\n",
      "  batch 200 loss: 0.4627914994955063\n",
      "  batch 210 loss: 0.4776129901409149\n",
      "LOSS train 0.46993195216789424 valid 0.4787030816078186\n",
      "EPOCH 14:\n",
      "  batch 10 loss: 0.46953419148921965\n",
      "  batch 20 loss: 0.4544205695390701\n",
      "  batch 30 loss: 0.4681194871664047\n",
      "  batch 40 loss: 0.4481921374797821\n",
      "  batch 50 loss: 0.47333013117313383\n",
      "  batch 60 loss: 0.4492552250623703\n",
      "  batch 70 loss: 0.4571261465549469\n",
      "  batch 80 loss: 0.4486409991979599\n",
      "  batch 90 loss: 0.44446619153022765\n",
      "  batch 100 loss: 0.4719754159450531\n",
      "  batch 110 loss: 0.4729027569293976\n",
      "  batch 120 loss: 0.4618313401937485\n",
      "  batch 130 loss: 0.4733815103769302\n",
      "  batch 140 loss: 0.46000866293907167\n",
      "  batch 150 loss: 0.4657383531332016\n",
      "  batch 160 loss: 0.45544442534446716\n",
      "  batch 170 loss: 0.4811213821172714\n",
      "  batch 180 loss: 0.48275561928749083\n",
      "  batch 190 loss: 0.4697636812925339\n",
      "  batch 200 loss: 0.4635228246450424\n",
      "  batch 210 loss: 0.46903358697891234\n",
      "LOSS train 0.46499345625672384 valid 0.4609566926956177\n",
      "EPOCH 15:\n",
      "  batch 10 loss: 0.47507147789001464\n",
      "  batch 20 loss: 0.4789225846529007\n",
      "  batch 30 loss: 0.4771683752536774\n",
      "  batch 40 loss: 0.44719758033752444\n",
      "  batch 50 loss: 0.45659648180007933\n",
      "  batch 60 loss: 0.447066193819046\n",
      "  batch 70 loss: 0.46006870865821836\n",
      "  batch 80 loss: 0.4784564673900604\n",
      "  batch 90 loss: 0.4840646356344223\n",
      "  batch 100 loss: 0.47179298996925356\n",
      "  batch 110 loss: 0.46290135383605957\n",
      "  batch 120 loss: 0.46623882949352263\n",
      "  batch 130 loss: 0.4776050508022308\n",
      "  batch 140 loss: 0.44861767292022703\n",
      "  batch 150 loss: 0.45568807125091554\n",
      "  batch 160 loss: 0.44531268775463106\n",
      "  batch 170 loss: 0.45421149730682375\n",
      "  batch 180 loss: 0.49079867601394656\n",
      "  batch 190 loss: 0.4728946387767792\n",
      "  batch 200 loss: 0.4667565256357193\n",
      "  batch 210 loss: 0.4680011451244354\n",
      "LOSS train 0.46600864939043457 valid 0.47104552388191223\n",
      "EPOCH 16:\n",
      "  batch 10 loss: 0.4714321821928024\n",
      "  batch 20 loss: 0.48723379969596864\n",
      "  batch 30 loss: 0.4670520812273026\n",
      "  batch 40 loss: 0.4657470643520355\n",
      "  batch 50 loss: 0.4570441633462906\n",
      "  batch 60 loss: 0.470894581079483\n",
      "  batch 70 loss: 0.46467509865760803\n",
      "  batch 80 loss: 0.48343886733055114\n",
      "  batch 90 loss: 0.46489439308643343\n",
      "  batch 100 loss: 0.4650081843137741\n",
      "  batch 110 loss: 0.48009319603443146\n",
      "  batch 120 loss: 0.4676611304283142\n",
      "  batch 130 loss: 0.4593564927577972\n",
      "  batch 140 loss: 0.47751063406467437\n",
      "  batch 150 loss: 0.4810780078172684\n",
      "  batch 160 loss: 0.4683372139930725\n",
      "  batch 170 loss: 0.4720682680606842\n",
      "  batch 180 loss: 0.4486691653728485\n",
      "  batch 190 loss: 0.47314574718475344\n",
      "  batch 200 loss: 0.45079662203788756\n",
      "  batch 210 loss: 0.4688184171915054\n",
      "LOSS train 0.4690753371916085 valid 0.49421605467796326\n",
      "EPOCH 17:\n",
      "  batch 10 loss: 0.4726048529148102\n",
      "  batch 20 loss: 0.4649183601140976\n",
      "  batch 30 loss: 0.4744018852710724\n",
      "  batch 40 loss: 0.4740565061569214\n",
      "  batch 50 loss: 0.46666165590286257\n",
      "  batch 60 loss: 0.46616741120815275\n",
      "  batch 70 loss: 0.4631622940301895\n",
      "  batch 80 loss: 0.4554764747619629\n",
      "  batch 90 loss: 0.4701975852251053\n",
      "  batch 100 loss: 0.46438839137554166\n",
      "  batch 110 loss: 0.4811099261045456\n",
      "  batch 120 loss: 0.4784148305654526\n",
      "  batch 130 loss: 0.47771073281764986\n",
      "  batch 140 loss: 0.4609531104564667\n",
      "  batch 150 loss: 0.4719024032354355\n",
      "  batch 160 loss: 0.4684059590101242\n",
      "  batch 170 loss: 0.46155235171318054\n",
      "  batch 180 loss: 0.47303229868412017\n",
      "  batch 190 loss: 0.4617442011833191\n",
      "  batch 200 loss: 0.4632279008626938\n",
      "  batch 210 loss: 0.46989006400108335\n",
      "LOSS train 0.4682637830482465 valid 0.4703536629676819\n",
      "EPOCH 18:\n",
      "  batch 10 loss: 0.4602472990751266\n",
      "  batch 20 loss: 0.45828803479671476\n",
      "  batch 30 loss: 0.4568567305803299\n",
      "  batch 40 loss: 0.4903486013412476\n",
      "  batch 50 loss: 0.4676926612854004\n",
      "  batch 60 loss: 0.47559828460216524\n",
      "  batch 70 loss: 0.4582116097211838\n",
      "  batch 80 loss: 0.46937360167503356\n",
      "  batch 90 loss: 0.47058957517147065\n",
      "  batch 100 loss: 0.4792521059513092\n",
      "  batch 110 loss: 0.4622368037700653\n",
      "  batch 120 loss: 0.47680549919605253\n",
      "  batch 130 loss: 0.46504458487033845\n",
      "  batch 140 loss: 0.46677969098091127\n",
      "  batch 150 loss: 0.4872255861759186\n",
      "  batch 160 loss: 0.45865899622440337\n",
      "  batch 170 loss: 0.4675074428319931\n",
      "  batch 180 loss: 0.4641112297773361\n",
      "  batch 190 loss: 0.47287810742855074\n",
      "  batch 200 loss: 0.4618161916732788\n",
      "  batch 210 loss: 0.45445762276649476\n",
      "LOSS train 0.4679916971754805 valid 0.4629329741001129\n",
      "EPOCH 19:\n",
      "  batch 10 loss: 0.45666074454784394\n",
      "  batch 20 loss: 0.4653263449668884\n",
      "  batch 30 loss: 0.47791760563850405\n",
      "  batch 40 loss: 0.4731181800365448\n",
      "  batch 50 loss: 0.46350337266922\n",
      "  batch 60 loss: 0.47822293639183044\n",
      "  batch 70 loss: 0.4661742329597473\n",
      "  batch 80 loss: 0.46599871218204497\n",
      "  batch 90 loss: 0.4653487354516983\n",
      "  batch 100 loss: 0.4621162533760071\n",
      "  batch 110 loss: 0.45412214994430544\n",
      "  batch 120 loss: 0.45971744060516356\n",
      "  batch 130 loss: 0.4597213685512543\n",
      "  batch 140 loss: 0.4552455484867096\n",
      "  batch 150 loss: 0.44531643986701963\n",
      "  batch 160 loss: 0.46024582386016843\n",
      "  batch 170 loss: 0.4767619729042053\n",
      "  batch 180 loss: 0.4545502424240112\n",
      "  batch 190 loss: 0.45694137513637545\n",
      "  batch 200 loss: 0.48540930151939393\n",
      "  batch 210 loss: 0.4542512834072113\n",
      "LOSS train 0.46361867512497945 valid 0.47887930274009705\n",
      "EPOCH 20:\n",
      "  batch 10 loss: 0.476820370554924\n",
      "  batch 20 loss: 0.46267777383327485\n",
      "  batch 30 loss: 0.4744656831026077\n",
      "  batch 40 loss: 0.47962530255317687\n",
      "  batch 50 loss: 0.4690108895301819\n",
      "  batch 60 loss: 0.4636858910322189\n",
      "  batch 70 loss: 0.46799922585487364\n",
      "  batch 80 loss: 0.4835235446691513\n",
      "  batch 90 loss: 0.4487180978059769\n",
      "  batch 100 loss: 0.48060835599899293\n",
      "  batch 110 loss: 0.46831423342227935\n",
      "  batch 120 loss: 0.4789058655500412\n",
      "  batch 130 loss: 0.4633904695510864\n",
      "  batch 140 loss: 0.46841845512390134\n",
      "  batch 150 loss: 0.467200380563736\n",
      "  batch 160 loss: 0.477804160118103\n",
      "  batch 170 loss: 0.46833177506923673\n",
      "  batch 180 loss: 0.47242942452430725\n",
      "  batch 190 loss: 0.46049049496650696\n",
      "  batch 200 loss: 0.44578735530376434\n",
      "  batch 210 loss: 0.4519456893205643\n",
      "LOSS train 0.46750624450010675 valid 0.4555220901966095\n",
      "EPOCH 21:\n",
      "  batch 10 loss: 0.4391054570674896\n",
      "  batch 20 loss: 0.44796675741672515\n",
      "  batch 30 loss: 0.47829519808292387\n",
      "  batch 40 loss: 0.48203028440475465\n",
      "  batch 50 loss: 0.47516647577285764\n",
      "  batch 60 loss: 0.4654814928770065\n",
      "  batch 70 loss: 0.4663224995136261\n",
      "  batch 80 loss: 0.4761742889881134\n",
      "  batch 90 loss: 0.4609257161617279\n",
      "  batch 100 loss: 0.4632883220911026\n",
      "  batch 110 loss: 0.45905380249023436\n",
      "  batch 120 loss: 0.46028720736503603\n",
      "  batch 130 loss: 0.4600068360567093\n",
      "  batch 140 loss: 0.4608442336320877\n",
      "  batch 150 loss: 0.46428648233413694\n",
      "  batch 160 loss: 0.45761160254478456\n",
      "  batch 170 loss: 0.4705867111682892\n",
      "  batch 180 loss: 0.4661982238292694\n",
      "  batch 190 loss: 0.47274625301361084\n",
      "  batch 200 loss: 0.45861547291278837\n",
      "  batch 210 loss: 0.45893478095531465\n",
      "LOSS train 0.46446899916524087 valid 0.4947546124458313\n",
      "EPOCH 22:\n",
      "  batch 10 loss: 0.46994111835956576\n",
      "  batch 20 loss: 0.48138115406036375\n",
      "  batch 30 loss: 0.4615448653697968\n",
      "  batch 40 loss: 0.472662752866745\n",
      "  batch 50 loss: 0.4630957871675491\n",
      "  batch 60 loss: 0.45983206033706664\n",
      "  batch 70 loss: 0.45911322832107543\n",
      "  batch 80 loss: 0.45320056676864623\n",
      "  batch 90 loss: 0.4516091048717499\n",
      "  batch 100 loss: 0.44476442933082583\n",
      "  batch 110 loss: 0.48940521776676177\n",
      "  batch 120 loss: 0.4754211336374283\n"
     ]
    }
   ],
   "source": [
    "!python ./scripts/train_nlp_bert_sm_compatible.py --epochs 100 --model_id \"bert-base-multilingual-uncased\" --training_dir \"./dataset\" --output_dir \"./model\" --learning_rate 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03db9d74-00e7-4909-9165-6274f182b892",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
