{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cca30dfe-50ee-4fa7-9dd3-bee9a761c78f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker.huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c71b2bbb-636f-4ce3-baf0-d951e989a36a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::707684582322:role/service-role/AmazonSageMaker-ExecutionRole-20191024T163188\n",
      "sagemaker bucket: sagemaker-eu-west-1-707684582322\n",
      "sagemaker session region: eu-west-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0789553-9676-40e2-a15b-0558bb19b188",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-eu-west-1-707684582322/DEMO-huggingface-finetune-twitter/train'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "prefix = 'DEMO-huggingface-finetune-twitter'\n",
    "s3_input_train_validation = 's3://{}/{}/train'.format(sagemaker_session_bucket, prefix)\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(sagemaker_session_bucket).Object(os.path.join(prefix, 'train/sem_eval_2018_task_1_train.csv')).upload_file('./dataset/sem_eval_2018_task_1_train.csv')\n",
    "boto3.Session().resource('s3').Bucket(sagemaker_session_bucket).Object(os.path.join(prefix, 'train/sem_eval_2018_task_1_validation.csv')).upload_file('./dataset/sem_eval_2018_task_1_validation.csv')\n",
    "\n",
    "s3_input_train_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84c201ba-19eb-42bb-aeb6-a91b1d06b84d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m \u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m \u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Dataset, DataLoader, random_split\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m BertTokenizer, BertModel, BertConfig\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctional\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mF\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatetime\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m datetime\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[35min\u001b[39;49;00m os.environ:\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mpass\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34melse\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "    os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[33m\"\u001b[39;49;00m\u001b[33m./\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[33m\"\u001b[39;49;00m\u001b[33m./\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mMyNLPDataset\u001b[39;49;00m(Dataset):\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, file_name, model_name):\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# data loading\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        df = pd.read_csv(file_name)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.data_en = df.loc[:,[\u001b[33m\"\u001b[39;49;00m\u001b[33mTweet\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]].squeeze().values.tolist()\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.label_index = torch.from_numpy(df.iloc[:,\u001b[34m2\u001b[39;49;00m:].astype(\u001b[36mint\u001b[39;49;00m).to_numpy())\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.n_samples = df.shape[\u001b[34m0\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.embedding_model = \u001b[34mNone\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.tokenizer = \u001b[34mNone\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.model_name = model_name\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.tokenized_text = [ {} \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(df.shape[\u001b[34m0\u001b[39;49;00m])]\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m#self.token_type_ids = None\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m#self.attention_mask = None\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__getitem__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, index):\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.tokenized_text[index] == {}:\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.tokenizer == \u001b[34mNone\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[36mself\u001b[39;49;00m.load_embedding_model()\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[37m\u001b[39;49;00m\n",
      "            text = \u001b[36mself\u001b[39;49;00m.data_en[index]\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34massert\u001b[39;49;00m \u001b[36misinstance\u001b[39;49;00m(text, \u001b[36mstr\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "            text_tokenized = \u001b[36mself\u001b[39;49;00m.tokenize_function(text)\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[36mself\u001b[39;49;00m.tokenized_text[index] = text_tokenized\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m#return self.tokenized_text[index]\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.tokenized_text[index], \u001b[36mself\u001b[39;49;00m.label_index[index]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__len__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.n_samples\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mload_embedding_model\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.tokenizer = BertTokenizer.from_pretrained(\u001b[36mself\u001b[39;49;00m.model_name)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mtokenize_function\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m,text):\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# generate token from dataset \u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        tokenized_text = \u001b[36mself\u001b[39;49;00m.tokenizer(text, max_length = \u001b[34m128\u001b[39;49;00m, padding=\u001b[33m\"\u001b[39;49;00m\u001b[33mmax_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, return_tensors=\u001b[33m\"\u001b[39;49;00m\u001b[33mpt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        tokenized_text[\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = torch.squeeze(tokenized_text[\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "        tokenized_text[\u001b[33m'\u001b[39;49;00m\u001b[33mtoken_type_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = torch.squeeze(tokenized_text[\u001b[33m'\u001b[39;49;00m\u001b[33mtoken_type_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "        tokenized_text[\u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = torch.squeeze(tokenized_text[\u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m tokenized_text\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# PyTorch models inherit from torch.nn.Module\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mSentenceMultiClassClassifier\u001b[39;49;00m(nn.Module):\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m,number_class, pretrained_model):\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36msuper\u001b[39;49;00m(SentenceMultiClassClassifier, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.number_class = number_class\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.pretrained = BertModel.from_pretrained(pretrained_model)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m#self.pretrained = BertModel.from_pretrained(pretrained_model,config=AutoConfig.from_pretrained(pretrained_model, output_attentions=True,output_hidden_states=True))\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m#self.dropout = nn.Dropout(0.5) \u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m#self.fc1 = nn.Linear(768, 1200)\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m#self.fc2 = nn.Linear(1200, 1400)\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m#self.fc3 = nn.Linear(1400, number_class)\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.linear = nn.Linear(\u001b[34m768\u001b[39;49;00m, number_class)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.layeroutput = torch.nn.Sigmoid()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, input_ids, token_type_ids, attention_mask):            \u001b[37m\u001b[39;49;00m\n",
      "        output_pretrained = \u001b[36mself\u001b[39;49;00m.pretrained(input_ids, token_type_ids, attention_mask)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# Freeze the BERT parameters\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m#for param in self.pretrained.parameters():\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m#    param.requires_grad = False\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m#x = F.relu(self.fc1(output_pretrained.last_hidden_state[:,0,:].view(-1,768)))\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m#x = self.dropout(x)\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m#x = F.relu(self.fc2(x))\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m#x = self.dropout(x)\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m#x = output_pretrained.last_hidden_state[:,0,:].view(-1,768)\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m\u001b[39;49;00m\n",
      "        x = output_pretrained.pooler_output\u001b[37m\u001b[39;49;00m\n",
      "        x = \u001b[36mself\u001b[39;49;00m.linear(x)\u001b[37m\u001b[39;49;00m\n",
      "        x = \u001b[36mself\u001b[39;49;00m.layeroutput(x)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m x\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain_one_epoch\u001b[39;49;00m(epoch_index,training_loader, optimizer, model, loss_fn, device):\u001b[37m\u001b[39;49;00m\n",
      "    running_loss = \u001b[34m0.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    last_loss = \u001b[34m0.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    batch_loss = \u001b[34m0.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Here, we use enumerate(training_loader) instead of\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# iter(training_loader) so that we can track the batch\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# index and do some intra-epoch reporting\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m i, data \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(training_loader):\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# Every data instance is an input + label pair\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        inputs, labels = data\u001b[37m\u001b[39;49;00m\n",
      "        labels = labels.type(torch.FloatTensor)\u001b[37m\u001b[39;49;00m\n",
      "        labels = labels.to(device)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mfor\u001b[39;49;00m key, value \u001b[35min\u001b[39;49;00m inputs.items():\u001b[37m\u001b[39;49;00m\n",
      "            inputs[key] = inputs[key].to(device)\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# Zero your gradients for every batch!\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        optimizer.zero_grad()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# Make predictions for this batch\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        outputs = model(**inputs)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# Compute the loss and its gradients\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        loss = loss_fn(outputs, labels)\u001b[37m\u001b[39;49;00m\n",
      "        loss.backward()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# Adjust learning weights\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        optimizer.step()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# Gather data and report\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        running_loss += loss.item()\u001b[37m\u001b[39;49;00m\n",
      "        batch_loss += loss.item()\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m i % \u001b[34m10\u001b[39;49;00m == \u001b[34m9\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "            last_loss = running_loss / \u001b[34m10\u001b[39;49;00m \u001b[37m# loss per batch\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m  batch \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m loss: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(i + \u001b[34m1\u001b[39;49;00m, last_loss))\u001b[37m\u001b[39;49;00m\n",
      "            tb_x = epoch_index * \u001b[36mlen\u001b[39;49;00m(training_loader) + i + \u001b[34m1\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "            running_loss = \u001b[34m0.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "           \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m batch_loss / \u001b[36mlen\u001b[39;49;00m(training_loader)    \u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# hyperparameters sent by the client are passed as command-line arguments to the script.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m3\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m32\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.00001\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Data, model, and output directories\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--output_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--training_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "    args, _ = parser.parse_known_args()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mStart training ...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    NUM_CLASS = \u001b[34m11\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    PRETRAINED_MODEL = args.model_id \u001b[37m\u001b[39;49;00m\n",
      "    BATCH_SIZE = args.train_batch_size\u001b[37m\u001b[39;49;00m\n",
      "    LEARNING_RATE = args.learning_rate\u001b[37m\u001b[39;49;00m\n",
      "    EPOCHS = args.epochs\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    TRAIN_LOCATION = args.training_dir + \u001b[33m\"\u001b[39;49;00m\u001b[33m/sem_eval_2018_task_1_train.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[37m\u001b[39;49;00m\n",
      "    VALIDATION_LOCATION = args.training_dir + \u001b[33m\"\u001b[39;49;00m\u001b[33m/sem_eval_2018_task_1_validation.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    MODEL_SAVE_LOCATION = args.output_dir\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    device = torch.device(\u001b[33m'\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    train_dataset = MyNLPDataset(TRAIN_LOCATION, PRETRAINED_MODEL)\u001b[37m\u001b[39;49;00m\n",
      "    validation_dataset = MyNLPDataset(VALIDATION_LOCATION, PRETRAINED_MODEL)\u001b[37m\u001b[39;49;00m\n",
      "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=\u001b[34mTrue\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    validation_dataloader = DataLoader(dataset=validation_dataset, batch_size=BATCH_SIZE, shuffle=\u001b[34mFalse\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    model = SentenceMultiClassClassifier(NUM_CLASS, PRETRAINED_MODEL)    \u001b[37m\u001b[39;49;00m\n",
      "    model.to(device)    \u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    loss_fn = torch.nn.BCELoss()\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Optimizers specified in the torch.optim package\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Initializing in a separate cell so we can easily add more epochs to the same run\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    timestamp = datetime.now().strftime(\u001b[33m'\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mY\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mm\u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m_\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mH\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mM\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    epoch_number = \u001b[34m0\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    best_vloss = \u001b[34m1_000_000.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(EPOCHS):\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mEPOCH \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m:\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(epoch_number + \u001b[34m1\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# Make sure gradient tracking is on, and do a pass over the data\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        model.train(\u001b[34mTrue\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        avg_loss = train_one_epoch(epoch_number, train_dataloader, optimizer, model, loss_fn, device)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# We don't need gradients on to do reporting\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m#model.train(False)\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        model.eval()\u001b[37m\u001b[39;49;00m\n",
      "        running_vloss = \u001b[34m0.0\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mfor\u001b[39;49;00m i, vdata \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(validation_dataloader):\u001b[37m\u001b[39;49;00m\n",
      "            vinputs, vlabels = vdata\u001b[37m\u001b[39;49;00m\n",
      "            vlabels = vlabels.type(torch.FloatTensor)\u001b[37m\u001b[39;49;00m\n",
      "            vlabels = vlabels.to(device)\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mfor\u001b[39;49;00m key, value \u001b[35min\u001b[39;49;00m vinputs.items():\u001b[37m\u001b[39;49;00m\n",
      "                vinputs[key] = vinputs[key].to(device)\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mwith\u001b[39;49;00m torch.no_grad():\u001b[37m\u001b[39;49;00m\n",
      "                voutputs = model(**vinputs)\u001b[37m\u001b[39;49;00m\n",
      "            vloss = loss_fn(voutputs, vlabels)\u001b[37m\u001b[39;49;00m\n",
      "            running_vloss += vloss\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        avg_vloss = running_vloss / (i + \u001b[34m1\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mLOSS train \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m valid \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(avg_loss, avg_vloss))\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# Track best performance, and save the model's state\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m avg_vloss < best_vloss:\u001b[37m\u001b[39;49;00m\n",
      "            best_vloss = avg_vloss\u001b[37m\u001b[39;49;00m\n",
      "            model_path = \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/model.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(MODEL_SAVE_LOCATION)\u001b[37m\u001b[39;49;00m\n",
      "            torch.save(model.state_dict(), model_path)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        epoch_number += \u001b[34m1\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./scripts/train_nlp_bert_sm_compatible.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4722d8b4-89a3-464e-b766-43174719d9c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "import time\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'epochs': 100,                          # number of training epochs\n",
    "                 'train_batch_size': 64,               # batch size for training\n",
    "                 'learning_rate': 0.000001,                # learning rate used during training\n",
    "                 'model_id': \"bert-base-multilingual-uncased\", # pre-trained model\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb3416a4-8780-41c9-86bb-7e1d3f324b3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    }
   ],
   "source": [
    "# define Training Job Name \n",
    "job_name = f'huggingface-finetune-twitter{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'train_nlp_bert_sm_compatible.py', # fine-tuning script used in training jon\n",
    "    source_dir           = './scripts',       # directory where fine-tuning script is stored\n",
    "    instance_type        = 'ml.p3.2xlarge',   # instances type used for the training job\n",
    "    #instance_type        = 'local_gpu',   # instances type used for the training job    \n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    transformers_version = '4.6',           # the transformers version used in the training job\n",
    "    pytorch_version      = '1.7',           # the pytorch_version version used in the training job\n",
    "    py_version           = 'py36',            # the python version used in the training job\n",
    "    hyperparameters      = hyperparameters,   # the hyperparameter used for running the training job\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1594eb0-8f4f-40cb-83d7-dccc6e3f8de2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-finetune-twitter2023-03-16--2023-03-16-11-34-44-518\n"
     ]
    }
   ],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "\n",
    "data = {\n",
    "    #'train': train_local_location,\n",
    "    'train': s3_input_train_validation,\n",
    "}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02756223-6090-4db0-a91a-5b3a108e51be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
