{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca30dfe-50ee-4fa7-9dd3-bee9a761c78f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker.huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71b2bbb-636f-4ce3-baf0-d951e989a36a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0789553-9676-40e2-a15b-0558bb19b188",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "prefix = 'DEMO-huggingface-finetune-twitter'\n",
    "s3_input_train_validation = 's3://{}/{}/train'.format(sagemaker_session_bucket, prefix)\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(sagemaker_session_bucket).Object(os.path.join(prefix, 'train/sem_eval_2018_task_1_train.csv')).upload_file('./dataset/sem_eval_2018_task_1_train.csv')\n",
    "boto3.Session().resource('s3').Bucket(sagemaker_session_bucket).Object(os.path.join(prefix, 'train/sem_eval_2018_task_1_validation.csv')).upload_file('./dataset/sem_eval_2018_task_1_validation.csv')\n",
    "\n",
    "s3_input_train_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c201ba-19eb-42bb-aeb6-a91b1d06b84d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pygmentize ./scripts/train_nlp_bert_sm_compatible.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4722d8b4-89a3-464e-b766-43174719d9c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "import time\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'epochs': 20,                          # number of training epochs\n",
    "                 'train_batch_size': 64,               # batch size for training\n",
    "                 'learning_rate': 0.00001,                # learning rate used during training\n",
    "                 'model_id': \"bert-base-multilingual-uncased\", # pre-trained model\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3416a4-8780-41c9-86bb-7e1d3f324b3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define Training Job Name \n",
    "job_name = f'huggingface-finetune-twitter{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'train_nlp_bert_sm_compatible.py', # fine-tuning script used in training jon\n",
    "    source_dir           = './scripts',       # directory where fine-tuning script is stored\n",
    "    instance_type        = 'ml.p3.2xlarge',   # instances type used for the training job\n",
    "    #instance_type        = 'local_gpu',   # instances type used for the training job    \n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    transformers_version = '4.6',           # the transformers version used in the training job\n",
    "    pytorch_version      = '1.7',           # the pytorch_version version used in the training job\n",
    "    py_version           = 'py36',            # the python version used in the training job\n",
    "    hyperparameters      = hyperparameters,   # the hyperparameter used for running the training job\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1594eb0-8f4f-40cb-83d7-dccc6e3f8de2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "\n",
    "data = {\n",
    "    #'train': \"file://./dataset\",\n",
    "    'train': s3_input_train_validation,\n",
    "}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02756223-6090-4db0-a91a-5b3a108e51be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
